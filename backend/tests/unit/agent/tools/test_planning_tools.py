# tests/unit/agent/tools/test_planning_tools.py

import pytest
from typing import Any, Dict, List, Optional
from types import SimpleNamespace
from unittest.mock import MagicMock, AsyncMock
from pydantic import ValidationError

# Module under test
from app.agent.tools import planning_tools as ptools

# Dependencies to patch
from app.core.config import settings
from app.agent.schemas import (
    InitialPlan,
    CharacterArchetypeList,
    CharacterCastingDecision,
)
from app.agent.tools import data_tools as dtools

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

# 1. Unit test marker
# 2. No Tool Stubs: We want to test the REAL planning_tools functions,
#    not the mocks generated by the global fixtures.
pytestmark = [pytest.mark.unit, pytest.mark.no_tool_stubs]


# ---------------------------------------------------------------------------
# Helpers & Fixtures
# ---------------------------------------------------------------------------

@pytest.fixture
def mock_settings(monkeypatch):
    """Helper to patch settings dynamically."""
    def _configure(
        retrieval_policy="all",
        allow_wiki=True,  # <--- Argument name is allow_wiki
        allow_web=True,
        max_characters=32
    ):
        # Patch retrieval settings
        r_settings = SimpleNamespace(
            policy=retrieval_policy,
            allow_wikipedia=allow_wiki,  # Maps to allow_wikipedia attribute
            allow_web=allow_web
        )
        monkeypatch.setattr(settings, "retrieval", r_settings, raising=False)
        
        # Patch quiz settings
        q_settings = SimpleNamespace(max_characters=max_characters)
        monkeypatch.setattr(settings, "quiz", q_settings, raising=False)
        return r_settings
    return _configure

@pytest.fixture
def mock_analyze_topic(monkeypatch):
    """
    Mock the intent classification to control logic flow (is_media, domain, etc.)
    without relying on the actual NLTK/heuristic implementation.
    """
    def _set_result(
        normalized="NormCat",
        is_media=False,
        outcome_kind="types",
        creativity_mode="balanced",
        domain="general",
        names_only=False
    ):
        result = {
            "normalized_category": normalized,
            "is_media": is_media,
            "outcome_kind": outcome_kind,
            "creativity_mode": creativity_mode,
            "intent": "identify",
            "domain": domain,
            "names_only": names_only,
        }
        monkeypatch.setattr(ptools, "analyze_topic", lambda cat: result, raising=True)
        return result
    return _set_result


# ---------------------------------------------------------------------------
# 1. Internal Logic Tests
# ---------------------------------------------------------------------------

def test_policy_allows(mock_settings):
    # 1. Policy Off
    mock_settings(retrieval_policy="off")
    assert ptools._policy_allows("web", media_hint=True) is False

    # 2. Web Disabled
    mock_settings(retrieval_policy="all", allow_web=False)
    assert ptools._policy_allows("web", media_hint=True) is False

    # 3. Media Only - Hint True
    mock_settings(retrieval_policy="media_only", allow_web=True)
    assert ptools._policy_allows("web", media_hint=True) is True

    # 4. Media Only - Hint False
    mock_settings(retrieval_policy="media_only", allow_web=True)
    assert ptools._policy_allows("web", media_hint=False) is False

def test_get_domain_queries():
    # Media
    w, web = ptools._get_domain_queries("Matrix", "The Matrix", "movies", True)
    assert "characters in Matrix" in w
    
    # Music (90s)
    w, web = ptools._get_domain_queries("90s R&B", "90s R&B", "music_artists_acts", False)
    assert "1990s R&B musicians" in w
    
    # Sports
    w, web = ptools._get_domain_queries("NBA", "NBA", "sports_leagues_teams", False)
    assert "NBA teams" in w

    # Default
    w, web = ptools._get_domain_queries("Types of Cheese", "Types of Cheese", "food", False)
    assert "Official types for Types of Cheese" in w

def test_filter_and_cap_names(mock_settings):
    mock_settings(max_characters=3)
    
    raw = ["Good Name", "bad name", "Another-Good", "123", "Too Many", "Items"]
    
    # Case 1: Media (Apply heuristic filter + Cap)
    filtered = ptools._filter_and_cap_names(raw, is_media=True, names_only=False)
    # "bad name" (lowercase start) and "123" (no letters) should be removed? 
    # Heuristic: any(tok[:1].isupper() for tok in w[:2])
    assert "Good Name" in filtered
    assert "bad name" not in filtered
    assert len(filtered) <= 3

    # Case 2: Non-media (No heuristic filter, just cap)
    filtered_gen = ptools._filter_and_cap_names(raw, is_media=False, names_only=False)
    assert "bad name" in filtered_gen
    assert len(filtered_gen) == 3


# ---------------------------------------------------------------------------
# 2. Tool: plan_quiz
# ---------------------------------------------------------------------------

@pytest.mark.asyncio
async def test_plan_quiz_happy_path(mock_analyze_topic, mock_settings, monkeypatch):
    mock_analyze_topic(normalized="Normalized Cats")
    mock_settings()

    # Mock LLM response
    async def _fake_llm(**kwargs):
        return InitialPlan(
            title="Quiz: Cats",
            synopsis="Synopsis",
            ideal_archetypes=["A", "B"],
            ideal_count_hint=None
        )
    monkeypatch.setattr(ptools, "invoke_structured", _fake_llm, raising=True)

    plan = await ptools.plan_quiz.ainvoke({"category": "Cats"})
    
    assert plan.title == "Quiz: Cats"
    assert plan.ideal_archetypes == ["A", "B"]

@pytest.mark.asyncio
async def test_plan_quiz_skips_analysis_if_provided(monkeypatch):
    """If outcome_kind/creativity_mode are passed, analyze_topic should NOT be called."""
    
    # If analyze_topic is called, raise error
    def boom(_): raise RuntimeError("Should skip analysis")
    monkeypatch.setattr(ptools, "analyze_topic", boom, raising=True)

    # Mock LLM
    async def _fake_llm(**kwargs):
        return InitialPlan(title="T", synopsis="S", ideal_archetypes=["A"])
    monkeypatch.setattr(ptools, "invoke_structured", _fake_llm, raising=True)

    plan = await ptools.plan_quiz.ainvoke({
        "category": "Cats",
        "outcome_kind": "types",
        "creativity_mode": "balanced",
        "intent": "identify"
    })
    assert plan.title == "T"

@pytest.mark.asyncio
async def test_plan_quiz_canonical_override(mock_analyze_topic, monkeypatch):
    mock_analyze_topic(normalized="CanonCat")
    
    # Mock canonical sets
    monkeypatch.setattr(ptools, "canonical_for", lambda x: ["Alpha", "Beta"], raising=True)
    monkeypatch.setattr(ptools, "count_hint_for", lambda x: 2, raising=True)

    # LLM returns something else
    async def _fake_llm(**kwargs):
        return InitialPlan(title="T", synopsis="S", ideal_archetypes=["Wrong"])
    monkeypatch.setattr(ptools, "invoke_structured", _fake_llm, raising=True)

    plan = await ptools.plan_quiz.ainvoke({"category": "Input"})
    
    # Result should be overridden by canonicals
    assert plan.ideal_archetypes == ["Alpha", "Beta"]
    assert plan.ideal_count_hint == 2

@pytest.mark.asyncio
async def test_plan_quiz_fallback_on_exception(mock_analyze_topic, monkeypatch):
    mock_analyze_topic(normalized="FallbackCat")
    
    async def _boom(**_): raise RuntimeError("LLM Fail")
    monkeypatch.setattr(ptools, "invoke_structured", _boom, raising=True)

    plan = await ptools.plan_quiz.ainvoke({"category": "Input"})
    
    assert plan.title == "What FallbackCat Are You?"
    assert plan.synopsis.startswith("A fun quiz")
    assert plan.ideal_archetypes == []

@pytest.mark.asyncio
async def test_plan_quiz_ensure_title_default(mock_analyze_topic, monkeypatch):
    """If LLM returns empty title, ensure we generate a default one."""
    mock_analyze_topic(normalized="Norm", names_only=True)
    
    async def _empty_title(**_):
        return InitialPlan(title="", synopsis="S", ideal_archetypes=[])
    monkeypatch.setattr(ptools, "invoke_structured", _empty_title, raising=True)

    plan = await ptools.plan_quiz.ainvoke({"category": "Input"})
    
    # names_only=True -> "Which Norm Are You?"
    assert plan.title == "Which Norm Are You?"


# ---------------------------------------------------------------------------
# 3. Tool: generate_character_list
# ---------------------------------------------------------------------------

@pytest.mark.asyncio
async def test_generate_character_list_canonical_short_circuit(monkeypatch):
    """If canonical set exists, return immediately without LLM or Analysis."""
    monkeypatch.setattr(ptools, "canonical_for", lambda x: ["A", "B"], raising=True)
    
    # Ensure LLM not called
    async def boom(**_): raise RuntimeError("Don't call LLM")
    monkeypatch.setattr(ptools, "invoke_structured", boom, raising=True)

    res = await ptools.generate_character_list.ainvoke({"category": "Canon", "synopsis": "S"})
    assert res == ["A", "B"]

@pytest.mark.asyncio
async def test_generate_character_list_retrieval_flow(mock_analyze_topic, mock_settings, monkeypatch):
    """
    Test the flow:
    1. analyze_topic says 'is_media=True' -> trigger retrieval.
    2. Wiki is tried -> consumes slot -> returns result.
    3. Web is skipped because Wiki succeeded.
    """
    mock_analyze_topic(normalized="MediaProp", is_media=True)
    # FIX: Changed allow_wikipedia -> allow_wiki to match fixture signature
    mock_settings(allow_wiki=True, allow_web=True)
    
    # Mock data_tools inside the lazy import context
    # Since it's imported inside the function, we patch the module `app.agent.tools.data_tools`
    mock_wiki = MagicMock()
    mock_wiki.ainvoke = AsyncMock(return_value="Wiki Result")
    
    mock_web = MagicMock()
    mock_web.ainvoke = AsyncMock(return_value="Web Result")
    
    # We need to patch the actual dtools module functions/objects
    monkeypatch.setattr(dtools, "wikipedia_search", mock_wiki, raising=False)
    monkeypatch.setattr(dtools, "web_search", mock_web, raising=False)
    monkeypatch.setattr(dtools, "consume_retrieval_slot", lambda t, s: True, raising=False)

    # Mock LLM to check if search_context was passed
    captured = {}
    async def _spy_llm(messages, **kwargs):
        # messages is [System, Human]. Check Human for search_context
        captured["prompt"] = messages[0].content
        return CharacterArchetypeList(archetypes=["A", "B"])
    
    monkeypatch.setattr(ptools, "invoke_structured", _spy_llm, raising=True)

    await ptools.generate_character_list.ainvoke({"category": "Input", "synopsis": "S"})

    # Assertions
    mock_wiki.ainvoke.assert_called_once()
    mock_web.ainvoke.assert_not_called() # Skipped because Wiki worked

@pytest.mark.asyncio
async def test_generate_character_list_retrieval_fallback_to_web(mock_analyze_topic, mock_settings, monkeypatch):
    """
    Test the flow:
    1. is_media=True.
    2. Wiki fails (returns empty).
    3. Web is tried -> returns result.
    """
    mock_analyze_topic(normalized="MediaProp", is_media=True)
    # FIX: Changed allow_wikipedia -> allow_wiki to match fixture signature
    mock_settings(allow_wiki=True, allow_web=True)
    
    mock_wiki = MagicMock()
    mock_wiki.ainvoke = AsyncMock(return_value="") # Fail
    
    mock_web = MagicMock()
    mock_web.ainvoke = AsyncMock(return_value="Web Result")
    
    monkeypatch.setattr(dtools, "wikipedia_search", mock_wiki, raising=False)
    monkeypatch.setattr(dtools, "web_search", mock_web, raising=False)
    monkeypatch.setattr(dtools, "consume_retrieval_slot", lambda t, s: True, raising=False)

    async def _fake_llm(**kwargs): return CharacterArchetypeList(archetypes=["A"])
    monkeypatch.setattr(ptools, "invoke_structured", _fake_llm, raising=True)

    await ptools.generate_character_list.ainvoke({"category": "Input", "synopsis": "S"})

    mock_wiki.ainvoke.assert_called_once()
    mock_web.ainvoke.assert_called_once()

@pytest.mark.asyncio
async def test_generate_character_list_llm_fallback_parsing(mock_analyze_topic, monkeypatch):
    """
    Test logic:
    1. Primary invoke (CharacterArchetypeList) raises Exception.
    2. Fallback invoke (List[str]) is called.
    """
    mock_analyze_topic(normalized="Gen", is_media=False) # No retrieval
    
    call_count = {"val": 0}

    async def _flaky_llm(response_model, **kwargs):
        call_count["val"] += 1
        # First call: Strict model -> Fail
        if response_model is CharacterArchetypeList:
            raise RuntimeError("Parse Error")
        # Second call: TypeAdapter -> Success
        return ["Fallback", "Success"]

    monkeypatch.setattr(ptools, "invoke_structured", _flaky_llm, raising=True)

    res = await ptools.generate_character_list.ainvoke({"category": "Input", "synopsis": "S"})
    
    assert res == ["Fallback", "Success"]
    assert call_count["val"] == 2

@pytest.mark.asyncio
async def test_generate_character_list_primary_dict_coercion(mock_analyze_topic, monkeypatch):
    """
    Test logic: Primary invoke returns a Dict (not Pydantic object).
    It should be coerced to CharacterArchetypeList automatically.
    """
    mock_analyze_topic(normalized="Gen", is_media=False)

    async def _dict_returning_llm(**kwargs):
        # Simulating what happens if invoke_structured returns a dict
        # (though in reality invoke_structured usually handles validation,
        # the code has an explicit check `if not isinstance(resp, CharacterArchetypeList)`)
        return {"archetypes": ["Dict", "Works"]}

    monkeypatch.setattr(ptools, "invoke_structured", _dict_returning_llm, raising=True)

    res = await ptools.generate_character_list.ainvoke({"category": "Input", "synopsis": "S"})
    assert res == ["Dict", "Works"]


# ---------------------------------------------------------------------------
# 4. Tool: select_characters_for_reuse
# ---------------------------------------------------------------------------

@pytest.mark.asyncio
async def test_select_characters_for_reuse_happy_path(mock_analyze_topic, monkeypatch):
    mock_analyze_topic(normalized="Cat")

    async def _fake_llm(**kwargs):
        return CharacterCastingDecision(
            reuse=[{"ideal_name": "I", "existing_name": "E", "reason": "R"}],
            improve=[],
            create=["C"]
        )
    monkeypatch.setattr(ptools, "invoke_structured", _fake_llm, raising=True)

    res = await ptools.select_characters_for_reuse.ainvoke({
        "category": "C", 
        "ideal_archetypes": [], 
        "retrieved_characters": []
    })

    assert isinstance(res, CharacterCastingDecision)
    assert len(res.reuse) == 1
    assert res.create == ["C"]

@pytest.mark.asyncio
async def test_select_characters_for_reuse_exception(mock_analyze_topic, monkeypatch):
    mock_analyze_topic(normalized="Cat")

    async def _boom(**_): raise RuntimeError("Fail")
    monkeypatch.setattr(ptools, "invoke_structured", _boom, raising=True)

    res = await ptools.select_characters_for_reuse.ainvoke({
        "category": "C", 
        "ideal_archetypes": [], 
        "retrieved_characters": []
    })

    # Returns empty default
    assert isinstance(res, CharacterCastingDecision)
    assert res.reuse == []
    assert res.create == []